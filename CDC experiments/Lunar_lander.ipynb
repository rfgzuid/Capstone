{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from src.capstone.evaluation import Evaluator\n",
    "from src.capstone.cbf import CBF, InfeasibilityError\n",
    "from src.capstone.settings import Env\n",
    "from src.capstone.noise import LunarLanderNoise\n",
    "from src.capstone.nndm import NNDM\n",
    "from src.capstone.ddpg import Actor\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "from bound_propagation.polynomial import Pow\n",
    "from bound_propagation.linear import FixedLinear\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d0ad346f7944412"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We make a plot similar to the Quadruped example of the Robust Safety paper (figure 1). In this case, we apply a simple constraint: an x range $[-0.2, 0.2]$ in which the Lander must remain. This corresponds to the positions of the flags that mark the landing area.\n",
    "\n",
    "$$h(x) = 1 - \\frac{x^2}{0.2^2} = 1-25x^2$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "686247da318bd92f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# allows to set a bound (also on other values than x = 0.2)\n",
    "x_bound = 0.15"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34b7022b716e10f6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ContinuousLunarLander(Env):\n",
    "\n",
    "    def __init__(self, noise: list[float]) -> None:\n",
    "        env = gym.make(\"LunarLander-v2\", continuous=True)\n",
    "        \n",
    "        self.is_discrete = False\n",
    "\n",
    "        self.settings = {\n",
    "            'noise': {\n",
    "                'x': noise[0],\n",
    "                'y': noise[1],\n",
    "                'theta': noise[2],\n",
    "                'v_x': noise[3],\n",
    "                'v_y': noise[4],\n",
    "                'v_theta': noise[5]\n",
    "            },\n",
    "\n",
    "            'replay_size': 1_000_000,\n",
    "            'batch_size': 128,\n",
    "            'num_episodes': 3000,\n",
    "            'max_frames': 1000,  # so that the lander prioritizes landing quick\n",
    "\n",
    "            'gamma': 0.99,\n",
    "            'tau': 0.001,\n",
    "\n",
    "            'NNDM_layers': (64,),\n",
    "            'NNDM_activation': nn.Tanh,\n",
    "            'NNDM_criterion': nn.MSELoss,\n",
    "            'NNDM_optim': optim.Adam,\n",
    "            'NNDM_lr': 1e-3,\n",
    "\n",
    "            'Actor_layers': (256, 128, 64),\n",
    "            'Actor_activation': F.relu,\n",
    "            'Actor_optim': optim.AdamW,\n",
    "            'Actor_lr': 1e-4,\n",
    "            'Action_bound': 1.,  # action space is bounded to [-1, 1] - see gymnasium docs\n",
    "\n",
    "            'Critic_layers': {'s': (256, 128), 'a': (128,), 'concat': (128,)},\n",
    "            'Critic_activation': F.relu,\n",
    "            'Critic_criterion': nn.SmoothL1Loss,\n",
    "            'Critic_optim': optim.AdamW,\n",
    "            'Critic_lr': 1e-3,\n",
    "\n",
    "            'OU_mu': 0,\n",
    "            'OU_theta': 0.15,\n",
    "            'OU_sigma': 0.2\n",
    "        }\n",
    "\n",
    "        self.h_function = nn.Sequential(\n",
    "            FixedLinear(\n",
    "                torch.tensor([\n",
    "                    [1., 0, 0, 0, 0, 0, 0, 0]\n",
    "                ]),\n",
    "                torch.tensor([0.])\n",
    "            ),\n",
    "            Pow(2),\n",
    "            FixedLinear(\n",
    "                torch.tensor([\n",
    "                    [-1 / x_bound ** 2]\n",
    "                ]),\n",
    "                torch.tensor([1.])\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.h_ids = [0]\n",
    "        self.std = [noise[i] for i in self.h_ids]\n",
    "        self.env = LunarLanderNoise(env, self.settings['noise'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e15de7928b029d8b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# positions - 0.005, velocities - 0.025\n",
    "env = ContinuousLunarLander([0.025, 0.025, 0.05, 0.05, 0.025, 0.05])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "877adee4506914c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Alpha\n",
    "\n",
    "$$\\nabla^2 h = \\begin{bmatrix}\n",
    "-50\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$\\nabla^2 h^T \\ \\nabla^2 h = \\begin{bmatrix}\n",
    "2500\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$det(\\nabla^2 h^T \\ \\nabla^2 h - \\sigma I) = 2500 - \\sigma = 0$$\n",
    "\n",
    "$$\\sigma = 2500$$\n",
    "\n",
    "$$\\lambda_{max} = \\sqrt{\\sigma} = 50$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "831f9fd5046b35d6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "lambda_max = 50\n",
    "\n",
    "stds = [0.005, 0.005, 0.025, 0.025, 0.005, 0.025]\n",
    "tr_cov = sum(std ** 2 for std in stds)\n",
    "\n",
    "psi = (lambda_max / 2) * tr_cov\n",
    "alpha = 1 - psi\n",
    "\n",
    "print(f'Alpha is {round(alpha, 3)}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31777064c773a07f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NNDM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c59829bbf816d69"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "nndm = NNDM(env)\n",
    "nndm_params = torch.load(f'../models/NNDMs/{type(env).__name__}')\n",
    "nndm.load_state_dict(nndm_params)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dddb293fa7cb0f50"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Agent"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c838daa599def751"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "policy = Actor(env)\n",
    "policy_params = torch.load(f'../models/Agents/{type(env).__name__}')\n",
    "policy.load_state_dict(policy_params)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5e747cb97539b6f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a263bcb7540f9c97"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cbf = CBF(env, nndm, policy,\n",
    "          alpha=[alpha],\n",
    "          delta=[0.],\n",
    "          no_action_partitions=4,\n",
    "          no_noise_partitions=8,\n",
    "          stochastic=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0a0a20a2d8de28a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def mc_simulate(num_agents, cbf_enabled=False, seed=42):\n",
    "    trajectories = []\n",
    "    unsafe_frames = []\n",
    "\n",
    "    for _ in tqdm(range(num_agents)):\n",
    "        xy_list = []\n",
    "        state, _ = env.env.reset(seed=seed)\n",
    "        xy_list.append(state[:2])\n",
    "        \n",
    "        state = torch.tensor(state).unsqueeze(0)\n",
    "\n",
    "        current_frame = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if cbf_enabled:\n",
    "                try:\n",
    "                    action = cbf.safe_action(state)\n",
    "                except InfeasibilityError:\n",
    "                    action = policy.select_action(state, exploration=False)\n",
    "            else:\n",
    "                action = policy.select_action(state, exploration=False)\n",
    "\n",
    "            state, reward, terminated, truncated, _ = env.env.step(action.squeeze().detach().numpy())\n",
    "            xy_list.append(state[:2])\n",
    "            \n",
    "            state = torch.tensor(state).unsqueeze(0)\n",
    "\n",
    "            current_frame += 1\n",
    "\n",
    "            if torch.any(env.h_function(state.unsqueeze(0)) < 0).item():\n",
    "                unsafe_frames.append(current_frame)\n",
    "                terminated = True\n",
    "\n",
    "            done = terminated or truncated\n",
    "        trajectories.append(np.array(xy_list))\n",
    "\n",
    "    env.env.close()\n",
    "    return unsafe_frames, trajectories"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f92c3297947f50f9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def plot_paths(num_agents=50):\n",
    "    agent_frames, agent_trajectories = mc_simulate(num_agents)\n",
    "    cbf_frames, cbf_trajectories = mc_simulate(num_agents, cbf_enabled=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(9, 5))\n",
    "    \n",
    "    for agent_run in agent_trajectories:\n",
    "        ax[0].plot(*zip(*agent_run), color='r', alpha=0.1)\n",
    "    \n",
    "    for cbf_run in cbf_trajectories:\n",
    "        ax[1].plot(*zip(*cbf_run), color='g', alpha=0.1)\n",
    "    \n",
    "    ax[0].plot(*zip(agent_trajectories[0][0]), marker='o', color='black', markersize=5)\n",
    "    ax[1].plot(*zip(cbf_trajectories[0][0]), marker='o', color='black', markersize=5)\n",
    "        \n",
    "    ax[0].vlines([-x_bound, x_bound], 0., 1.5, colors='black', linestyles='dashed')\n",
    "    ax[1].vlines([-x_bound, x_bound], 0., 1.5, colors='black', linestyles='dashed')\n",
    "    \n",
    "    ax[0].set_xlim(-0.3, 0.3)\n",
    "    ax[1].set_xlim(-0.3, 0.3)\n",
    "    \n",
    "    ax[0].set_ylim(0., 1.5)\n",
    "    ax[1].set_ylim(0., 1.5)\n",
    "    \n",
    "    ax[0].set_title(f'No filter - {len(agent_frames)} violations')\n",
    "    ax[1].set_title(f'Stochastic CBF - {len(cbf_frames)} violations')\n",
    "    \n",
    "    plt.savefig('plots/LunarLander.png')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a36d832c31264ff"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_paths(50)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f404e9858549d99"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def title_plot(num_agents=100, time_steps=1000):\n",
    "    state0, _ = env.env.reset(seed=42)\n",
    "    state0_copy = state0\n",
    "    x0, y0 = state0[0], state0[1]\n",
    "    print(state0[0], state0[1])\n",
    "    xy_list = []\n",
    "    xyc_cbf_list = []\n",
    "    for i in range(num_agents):\n",
    "        local_xy_list = []\n",
    "        local_xyc_cbf_list = []\n",
    "        for t in range(time_steps):\n",
    "            # no cbf\n",
    "            state_old = state0\n",
    "            action = policy.select_action(torch.tensor(state_old, dtype=torch.float32), exploration=False)\n",
    "            state_new, reward, terminated, truncated, _ = env.env.step(action.squeeze().detach().numpy())\n",
    "            xy = (state_new[0].item(), state_new[1].item())\n",
    "            local_xy_list.append(xy)\n",
    "            state0 = state_new\n",
    "        xy_list.append(local_xy_list)\n",
    "        local_xy_list = []\n",
    "        \n",
    "        \n",
    "\n",
    "        infeasibility_counter = 0\n",
    "        for t in range(time_steps):    \n",
    "            # cbf\n",
    "\n",
    "            state_cbf_old = state0_copy\n",
    "            \n",
    "            # follow original actor if no feasible safe action exists\n",
    "            try:\n",
    "                action = cbf.safe_action(torch.tensor(state_cbf_old, dtype=torch.float32))\n",
    "            except InfeasibilityError:\n",
    "                action = policy.select_action(torch.tensor(state_cbf_old, dtype=torch.float32), exploration=False)\n",
    "                infeasibility_counter += 1\n",
    "            state_cbf_new = env.env.step(action.squeeze().detach().numpy())\n",
    "            xyc_cbf = (state_cbf_new[0][0], state_cbf_new[0][1], infeasibility_counter)\n",
    "            state_cbf_old = state_cbf_new\n",
    "            local_xyc_cbf_list.append(xyc_cbf)\n",
    "            \n",
    "        xyc_cbf_list.append(local_xyc_cbf_list)\n",
    "        local_xyc_cbf_list = []\n",
    "        \n",
    "    print(\"xy list: \", xy_list)   \n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2)\n",
    "    xs = [xy_list[i][0][0] for i in range(len(xy_list))]\n",
    "    ys = [xy_list[i][0][1] for i in range(len(xy_list))]    \n",
    "    print(\"XS, YS\")\n",
    "    print(xs, ys)\n",
    "    \n",
    "    for n in range(num_agents):\n",
    "        x = xs[n*time_steps : time_steps*(n + 1)]\n",
    "        y = ys[n*time_steps : time_steps*(n + 1)]\n",
    "        print(\"the current agent is: \", n, \"with x list: {}, y list: {}\".format(x, y))\n",
    "        axs[0].plot(x, y, color='blue')\n",
    "        \n",
    "    plt.show()\n",
    "    return xy_list, xyc_cbf_list\n",
    "            \n",
    "            \n",
    "    \n",
    "title_plot(20, 10)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d46ee0677a21b7e3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2806cd4b09be400d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
